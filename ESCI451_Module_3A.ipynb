{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Python for Earth Scientists\n",
    "\n",
    "These notebooks have been developed by Calum Chamberlain, Finnigan Illsley-Kemp and John Townend at [Victoria University of Wellington-Te Herenga Waka](https://www.wgtn.ac.nz) for use by Earth Science graduate students. \n",
    "\n",
    "The notebooks cover material that we think will be of particular benefit to those students with little or no previous experience of computer-based data analysis. We presume very little background in command-line or code-based computing, and have compiled this material with an emphasis on general tasks that a grad student might encounter on a daily basis. \n",
    "\n",
    "In 2021, this material will be delivered at the start of Trimester 1 in conjunction with [ESCI451 Active Earth](https://www.wgtn.ac.nz/courses/esci/451/2021/offering?crn=32176). Space and pandemic alert levels permitting, interested students not enrolled in ESCI451 are encouraged to come along too but please contact Calum, Finn, or John first.\n",
    "\n",
    "| Notebook | Contents | Data |\n",
    "| --- | --- | --- |\n",
    "| [1A](ESCI451_Module_1A.ipynb) | Introduction to programming, Python, and Jupyter notebooks | - |\n",
    "| [1B](ESCI451_Module_1B.ipynb) | Basic data types and variables, getting data, and plotting with Matplotlib | Geodetic positions |\n",
    "| [2A](ESCI451_Module_2A.ipynb) | More complex plotting, introduction to Numpy | Geodetic positions; DFDP-2B temperatures |\n",
    "| [2B](ESCI451_Module_2B.ipynb) | Using Pandas to load, peruse and plot data | Earthquake catalogue  |\n",
    "| **[3A](ESCI451_Module_3A.ipynb)** | **Working with Pandas dataframes** | **Geochemical data set; earthquake catalogue** |\n",
    "| [3B](ESCI451_Module_3B.ipynb) | Simple time series analysis using Pandas | Historical temperature records |\n",
    "| [4A](ESCI451_Module_4A.ipynb) | Making maps with Cartopy | Earthquake catalogue |\n",
    "| [4B](ESCI451_Module_4B.ipynb) | Working with gridded data | DEMs and Ashfall data |\n",
    "\n",
    "The content may change in response to students' questions or current events. Each of the four modules has been designed to take about three hours, with a short break between each of the two parts.\n",
    "\n",
    "Remember to run the first cell below to set up plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook\n",
    "\n",
    "1. More with Pandas dataframes\n",
    "   - Loading and summarising another dataset\n",
    "   - Working with subplots\n",
    "   - Regression\n",
    "   - Dates and times and datetimes\n",
    "   - Querying a dataframe using a function\n",
    "   - Applying functions to a dataframe\n",
    "2. Extension: more sophisticated analysis of a big dataset\n",
    "   \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and summarising another dataset\n",
    "\n",
    "Let's recap where we got to with Pandas in the previous notebook. We learnt how to obtain data from GeoNet and store it in .csv file and a Pandas dataframe, which we could then inspect and plot in various ways. \n",
    "\n",
    "We'll start this notebook by reviewing some of the ways in which we can inspect, describe and plot Pandas dataframes, and then focus on working with dates and times. The first step is to reload the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a geochemical data set obtained by a PhD student, Elliot Swallow, for volcanic rocks from Huckleberry Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geochem = pd.read_csv(\n",
    "    \"data/Edited Swallow et al J Petrol data for plotting.csv\",\n",
    "    index_col=\"Sample\")\n",
    "print(geochem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing a dataframe out gives us a rough look at the data and we've seen before how we can compute various statistics to explore the dataframe piece by piece. But there's another way of getting a nicely-formatted, informative summary of the whole dataframe and that is to use the `.describe()` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geochem.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How cool is that?!\n",
    "\n",
    "## Working with subplots\n",
    "\n",
    "What we can immediately see is that this dataset contains a lot of geochemical parameters (38!) and if we want to discover how they relate to each other we're going to have to do some more plotting. Ideally, it would be interesting to plot each parameter against every other one but this will get pretty overwhelming so we'll focus on a few interesting ones.\n",
    "\n",
    "To make our plots, we'll use the `plt.subplots()` syntax we've encountered previously, but which we haven't yet used to its full extent. As the name suggests, subplots let you make multiple plots in one. \n",
    "\n",
    "To start with, let's plot the first four major elements in the table ($TiO_2$, $Al_2O_3$, $Fe_2O_3 (T)$, $MnO$) against $Si0_2$.\n",
    "\n",
    "To do this we will need four axes.  We can make four axes using the `plt.subplots` command.  Lets get help for that function.  You can get to the docs for a given function by typing `function?`.  Run the cell below and you should see a pop-up of the docs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments that we care about at the moment at:\n",
    "- `nrows`, the number of rows of subplots\n",
    "- `ncols`, the number of columns of subplots\n",
    "- `sharex`, whether axes should have the same x-axis.  We will be having $SiO_2$ on all the x-axes, so we will set this to be `True`.\n",
    "\n",
    "Lets make a 2x2 grid of subplots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, sharex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we Ä«ncluded `sharex=True` in our command, which will ensure that all four panels have the same x-axis.\n",
    "\n",
    "This returned two things as usual, `fig`: the `figure` containing all our axes, and `ax`, which is a list of the subplot axes.  We can specify which axes we want to use for each graph by remembering that Python counts from zero and noting that _the axes are indexed by row and then by column_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, sharex=True)\n",
    "for column in range(2):\n",
    "    for row in range(2):\n",
    "        ax[row][column].set_title(f\"ax[{row}][{column}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, let's plot actual data using two loops to run through the parameters and axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, sharex=True)\n",
    "\n",
    "elements = [[\"TiO2 (wt%)\", \"Al2O3 (wt%)\"], [\"Fe2O3 (T) (wt%)\", \"MnO (wt%)\"]]\n",
    "for column in range(2):\n",
    "    for row in range(2):\n",
    "        element_name = elements[row][column]\n",
    "        ax[row][column].scatter(geochem[\"SiO2 (wt%)\"], geochem[element_name])\n",
    "        ax[row][column].set_ylabel(element_name)\n",
    "        ax[row][column].set_xlabel(\"$SiO_2$ (wt%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The y-axis labels are getting in the way a bit but we can easily avoid this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, sharex=True)\n",
    "\n",
    "elements = [[\"TiO2 (wt%)\", \"Al2O3 (wt%)\"], [\"Fe2O3 (T) (wt%)\", \"MnO (wt%)\"]]\n",
    "for column in range(2):\n",
    "    for row in range(2):\n",
    "        element_name = elements[row][column]\n",
    "        ax[row][column].scatter(geochem[\"SiO2 (wt%)\"], geochem[element_name])\n",
    "        ax[row][column].set_ylabel(element_name)\n",
    "        ax[row][column].set_xlabel(\"$SiO_2$ (wt%)\")\n",
    "        if column == 1:\n",
    "            ax[row][column].yaxis.tick_right()\n",
    "            ax[row][column].yaxis.set_label_position(\"right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Try writing some code to plot the four parameters of interest against $SiO_2$ in a 1x4 subplot layout with a common y-axis ($SiO_2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "We can infer from the plots above that $Fe_2O_3$ and $TiO_2$ exhibit very similar behaviour with regard to $SiO_2$. Sure enough, we find when we plot one against the other that they have are highly correlated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geochem.plot(x=\"Fe2O3 (T) (wt%)\",y=\"TiO2 (wt%)\",kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use these two parameters to briefly explore linear regression. There are myriad ways to do this in Python but perhaps the simplest to start with is numpy's \"polyfit' algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "model, covariance = np.polyfit(geochem['Fe2O3 (T) (wt%)'],geochem['TiO2 (wt%)'],1, cov=True)\n",
    "print(model)\n",
    "print(covariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we've done is fit a linear model of the form $y=ax+b$ to the Fe (x) and Ti (y) oxide dat, using least-squares. We could have fit a quadratic model by simply changing the \"1\" at the end of the command to \"2\".\n",
    "\n",
    "We can use the \"poly1d\" function to construct a function from the model parameters that, when re-applied to the Fe data, will return the predicted Ti measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_function=np.poly1d(model)\n",
    "Ti_predicted=geochem['Fe2O3 (T) (wt%)'].apply(prediction_function) # Getting an assertion error here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results to see what we've found and whether it makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots()\n",
    "ax.plot(geochem['Fe2O3 (T) (wt%)'],geochem['TiO2 (wt%)'],'o',color='red',label='Measurements')\n",
    "ax.plot(geochem['Fe2O3 (T) (wt%)'],Ti_predicted,label='Linear model',linestyle='dotted',linewidth=0.5,color='red')\n",
    "ax.set_xlabel(\"Fe2O3 (T) (wt%)\")\n",
    "ax.set_ylabel(\"TiO2 (wt%)\")\n",
    "ax.set_title(\"An example of linear regression with numpy\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on dates and times and datetimes\n",
    "\n",
    "In the previous notebook, we made our GeoNet query using `str` objects for the start and end-time arguments, but Python has nice native ways of working with dates and times. These do useful things like cope with leap-years, allow you to add seconds (or other time units) to dates and times, and allow you to format dates and times as `str` objects.\n",
    "\n",
    "We should switch from giving `str`s to giving `datetime`s.  `datetime` objects come from Python's native `datetime` library, and include a handy `.strftime` method which is literally string-format-time.  We will use that to make the correctly formatted string for our query.  The query requires something of the format:\n",
    "\n",
    "> year-month-dayThour:minute:second\n",
    "\n",
    "In `datetime` speak the format string for that is:\n",
    "\n",
    "> `%Y-%m-%dT%H:%M:%S`\n",
    "\n",
    "- `%Y` is a four-digit year (e.g.: ..., 2018, 2019, 2020, ...)\n",
    "- `%m` is a two-digit month (e.g.: 01, 02, 03, 04, 05, 06, 07, 08, 09, 10, 11, 12)\n",
    "- `%d` is a two-digit day (zero-padded as months are, e.g.: 01, 02, ... 28, 29, 30, 31)\n",
    "- `T` is just a letter, anything not preceded by a `%` sign is interpreted as a `str`\n",
    "- `%H` is a two-digit hour (as above)\n",
    "- `%M` is a two-digit minute (as above)\n",
    "- `%S` is a two-digit second (as above).\n",
    "\n",
    "Other formatters, for things like day of the week, month as a word, julian-day, milliseconds, etc. can be found in the [offical docs](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes).\n",
    "\n",
    "Lets see how we could format a `datetime`.  To start we need to make a `datetime` object, we can provide arguments of the year, month, day, hour, minute, second, (millisecond) to make one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime is the module, datetime.datetime is the object itself\n",
    "test_time = datetime.datetime(2020, 1, 10, 12, 43, 10)\n",
    "print(test_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets format the string the way that we want it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_time.strftime(\"%Y-%m-%dT%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** format the `datetime` object as \"year/month/day hour:minute:seconds\" and then try some other formats using the information listed in the [offical docs](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how it all works, lets put it together into a function with some useful\n",
    "arguments that we can query.\n",
    "\n",
    "## Querying a dataframe using a function\n",
    "\n",
    "We will set some default values for our arguments, so that we do not always have to\n",
    "specify every argument.  These defaults are given in the function definition as:\n",
    "\n",
    "```python\n",
    "def function(argument=value, ...):\n",
    "    contents_of_function\n",
    "```\n",
    "where `argument` is the argument name, and `value` is the default value for that argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geonet_quakes(\n",
    "    min_latitude=-49.0, max_latitude=-40.0,\n",
    "    min_longitude=164.0, max_longitude=182.0,\n",
    "    min_magnitude=1, max_magnitude=9.0,\n",
    "    min_depth=0.0, max_depth=500.0,\n",
    "    start_time=datetime.datetime(1960, 1, 1),\n",
    "    end_time=datetime.datetime(2020, 1, 1),\n",
    "):\n",
    "    \"\"\"\n",
    "    Get a dataframe of the eatrhquakes in the GeoNet catalogue.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    min_latitude\n",
    "        Minimum latitude in degrees for search\n",
    "    max_latitude\n",
    "        Maximum latitude in degrees for search\n",
    "    min_longitude\n",
    "        Minimum longitude in degrees for search\n",
    "    max_longitude\n",
    "        Maximum longitude in degrees for search\n",
    "    min_depth\n",
    "        Minimum depth in km for search\n",
    "    max_depth\n",
    "        Maximum depth in km for search\n",
    "    min_magnitude\n",
    "        Minimum magnitude for search\n",
    "    max_magnitude\n",
    "        Maximum magnitude for search\n",
    "    start_time\n",
    "        Start date and time for search\n",
    "    end_time\n",
    "        End date and time for search\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DateFrame of resulting events\n",
    "    \"\"\"\n",
    "    # Convert start_time and end_time to strings\n",
    "    start_time = start_time.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    end_time = end_time.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    # Use the more efficient f-string formatting\n",
    "    query_string = (\n",
    "        \"https://quakesearch.geonet.org.nz/csv?bbox=\"\n",
    "        f\"{min_longitude},{min_latitude},{max_longitude},\"\n",
    "        f\"{max_latitude}&minmag={min_magnitude}\"\n",
    "        f\"&maxmag={max_magnitude}&mindepth={min_depth}\"\n",
    "        f\"&maxdepth={max_depth}&startdate={start_time}\"\n",
    "        f\"&enddate={end_time}\")\n",
    "    print(f\"Using query: {query_string}\")\n",
    "    response = requests.get(query_string)\n",
    "    with open(\"data/earthquakes.csv\", \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    earthquakes = pd.read_csv(\n",
    "        \"data/earthquakes.csv\", \n",
    "        parse_dates=[\"origintime\", \"modificationtime\"],\n",
    "        dtype={\"publicid\": str})\n",
    "    earthquakes = earthquakes.rename(\n",
    "        columns={\" magnitude\": \"magnitude\",\n",
    "                 \" latitude\": \"latitude\",\n",
    "                 \" depth\": \"depth\"})\n",
    "    return earthquakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets quickly run this function to get the data.  There won't be any output.  Note that I didn't\n",
    "provide these data in the repository because:\n",
    "1. I don't have permission to re-distribute the data and,\n",
    "2. this dataset gets updated frequently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquakes = get_geonet_quakes(\n",
    "    start_time=datetime.datetime(2015, 1, 1),min_magnitude=3)\n",
    "\n",
    "print(earthquakes.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do something useful: applying functions to a dataframe\n",
    "\n",
    "We saw in the previous notebook how we could sort and slice a dataframe in various ways.Let's look at\n",
    "an example of doing some actual calculations with a dataframe.  We will use some of what we have\n",
    "learnt to calculate the occurance rate of earthquakes within a region.  In this case\n",
    "we will take the region around the top of South Island, containing the faults that ruptured\n",
    "in the Kaikoura earthquake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaikoura = get_geonet_quakes(\n",
    "    min_latitude=-43.12, max_latitude=-41.15,\n",
    "    min_longitude=172.37, max_longitude=174.95,\n",
    "    start_time=datetime.datetime(2010, 1, 1),\n",
    "    min_magnitude=2)\n",
    "# Rename those columns\n",
    "kaikoura = kaikoura.rename(\n",
    "    columns={\" magnitude\": \"magnitude\",\n",
    "             \" latitude\": \"latitude\",\n",
    "             \" depth\": \"depth\"})\n",
    "\n",
    "print(f\"Downloaded {len(kaikoura)} earthquakes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earthquake rate is the number of earthquakes per unit time.  For a dataset like this we can calculate\n",
    "rate as 1 over the inter-event time.\n",
    "\n",
    "First we need to sort by origin time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaikoura = kaikoura.sort_values(by=[\"origintime\"], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to calculate the time between each successive earthquake.  We can do this by\n",
    "taking the `origintime` column away from a one-sample shifted version of the `origintime` column,\n",
    "much like we did for calculating the temperature gradient in the DFDP drillhole. In Pandas we\n",
    "can do this quickly using the `.diff` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_event_time = kaikoura[\"origintime\"].diff()\n",
    "print(inter_event_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This column is in `timedelta` format. Lets convert it to seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_event_time = inter_event_time.dt.total_seconds()\n",
    "print(inter_event_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rate is simply 1 / `inter_event_time`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = 1 / inter_event_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add this column into the dataframe and call it `\"rate\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaikoura['rate'] = rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot rate as a function of time. Remember that the time values we use here are the\n",
    "event origin-times, which do not directly represent the time of the rate calculated, which\n",
    "is an average between events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = kaikoura.plot(x=\"origintime\", y=\"rate\")\n",
    "ax.set_ylabel(\"Instantaneous Rate (earthquakes per second)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is lots more that you can do with dataframes.  Pandas is in heavy use in the datascience\n",
    "world, and allows you to quickly explore datasets.  Hopefully you will find it useful at some\n",
    "point in your Geoscience career.\n",
    "\n",
    "As a final thing, you can save your dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaikoura.to_csv(\"data/kaikoura.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "That covers *some* of the basics of `dataframes` in pandas.  You should be able to use them to replace most of what you would have done with spreadsheets to allow you to work in a more programatic and reproducible way. We also demonstrated some of the basics of fitting simple polynomials to data.  Note that numpy's `polyfit` function isn't limited to 1st order polynomials, the sky is the limit!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension: more sophisticated analysis of a dataframe\n",
    "\n",
    "### Extension: computing b-values\n",
    "\n",
    "Earthquake generally follow a Gutenberg-Richter relationship, where the logarithm of the cumulative number of earthquakes above a given magnitude is proportional to the magnitude:\n",
    "\\begin{equation}\n",
    "    \\log_{10}{N} = a - bM\n",
    "\\end{equation}\n",
    "where *M* is magnitude, *N* is the number of events with magnitude >= *M*, and *a* and *b* are constants. This is a nice simple straight-line equation with offset from the origin given by *a* and the gradient by *b*.\n",
    "\n",
    "Some studies (for example, [Nuannin et al., 2005](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2005GL022679)) have found variations in b-value with time and space, and related this to changes in stress.  Lets see if we can:\n",
    "1. Calculate the b-value for our dataset;\n",
    "2. Do some sliding-window fu to get at b-value variations in time.\n",
    "\n",
    "To kick us off, note that in our analysis we are going to miss one fundamental thing which means that everything we do is wrong.  That thing is catalogue completeness, upon which our b-value calculations depend. To show that completeness, and have a first pass at computing b-values, lets look at a cumulative distribution of earthquake magnitudes.\n",
    "\n",
    "### Plotting cumulative distributions\n",
    "\n",
    "We want an inverse cumulative plot of magnitudes. We can do this with matplotlib's `hist` by setting the `cumulative` argument to `-1`, and the `density` argument set to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(\n",
    "    kaikoura[\"magnitude\"], bins=len(kaikoura), \n",
    "    histtype=\"step\", density=True, log=True, \n",
    "    cumulative=-1)\n",
    "ax.set_xlabel(\"Magnitude\")\n",
    "ax.set_ylabel(\"Cumulative density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty straight on a log-normal plot, as we would expect from the Gutenberg-Richter law.  However, somewhere between M 2 and 3 it stops being straight.  We assume that our catalogue completeness is somewhere in here.  This means that we think that, if we could detect and catalogue all the earthquakes all the way down to the tiny earthquakes, we would continue seeing this log-normal relationship. So, we assume that below our magnitude of completeness ($M_C$) we are missing earthquakes.  This seems reasonable, as earthquakes get smaller they get much harder to detect simply because their amplitudes are greatly reduced.\n",
    "\n",
    "Lets *assume* our catalogue is complete to $M_C=2.5$ and try and fit a straight line to our cumulative-density plot.\n",
    "\n",
    "First we will count how many times each magnitude appears in our dataset, we will use a handy object in Pythons native `collections` library, called `Counter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counted_magnitudes = Counter(kaikoura[\"magnitude\"])\n",
    "\n",
    "print(counted_magnitudes.most_common(10))  # Print the most common 10 magnitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, that gives us a list of the magnitude and the number of occurrences of that magnitude.  What we actually want is magnitudes and the number of occurrences of that magnitude and *any magnitude above that magnitude*.  To do that we will:\n",
    "1. create a unique set of all the magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitudes = set(kaikoura[\"magnitude\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Make a sorted `list` from this set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitudes = sorted(list(magnitudes), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Remove all magnitudes below our completeness using a [list comprehension](https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions) (because they are handy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitudes = [m for m in magnitudes if m >= 2.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Initialise an empty array in which we will put the cumulative density function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "density = np.zeros(len(magnitudes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Loop through the magnitudes from largest to smallest and add the number of occurrences of that magnitude to the total occurrences of the previous magnitude bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density[0] = counted_magnitudes[magnitudes[0]]\n",
    "for i, magnitude in enumerate(magnitudes[1:]):\n",
    "    density[i + 1] = density[i] + counted_magnitudes[magnitude]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check that that looks okay by plotting it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(magnitudes, density)\n",
    "ax.set_ylabel(\"Cumulative density\")\n",
    "ax.set_xlabel(\"Magnitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! Now lets try and fit a line to it.  We can use `numpy`'s solvers to do this. Because this is a nice\n",
    "simple equation we will use the [numpy.polyfit](https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients, residual, rank, singular_values, rcondition = np.polyfit(\n",
    "    magnitudes, np.log10(density), deg=1, full=True)\n",
    "b, a = coefficients\n",
    "print(f\"a={a:.2f}, b={b:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b is usually close to 1 (note that the gradient calculated above is negative, which is already taken care of in the Gutenberg-Richter law). \n",
    "\n",
    "Lets estimate the density from our calculated values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make our lives easier we will convert our magnitudes to a numpy array:\n",
    "magnitudes = np.array(magnitudes)\n",
    "estimated_density = 10 ** (a + (magnitudes * b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, lets see if it fits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(\n",
    "    magnitudes, density, marker=\"+\", linestyle=\"None\",\n",
    "    label=\"Data\")\n",
    "ax.semilogy(magnitudes, estimated_density, label=\"Model\")\n",
    "ax.set_ylabel(\"Cumulative density\")\n",
    "ax.set_xlabel(\"Magnitude\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that because we specified `full=True` in our call to `polyfit`, we were returned a range of metrics about how well-fitted our data were.  The easiest one of those to understand is the residual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a measure of the misfit between our model and our data.\n",
    "\n",
    "Lets build a simple function to do this with the aim of applying this to distinct time-chunks of our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_b_value(magnitudes, completeness_magnitude=2.5):\n",
    "    \"\"\"\n",
    "    Calculate the b-value for a range of magnitudes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    magnitudes\n",
    "        List or array of magnitudes\n",
    "    completeness_magnitude\n",
    "        Magnitude of completeness for the dataset\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    b-value\n",
    "    \"\"\"\n",
    "    counted_magnitudes = Counter(magnitudes)\n",
    "    magnitudes = sorted(list(set(magnitudes)), reverse=True)\n",
    "    magnitudes = np.array(magnitudes)\n",
    "    # Remove magnitudes less than completess\n",
    "    magnitudes = magnitudes[magnitudes >= completeness_magnitude]\n",
    "    # Calculate density\n",
    "    density = np.zeros(len(magnitudes))\n",
    "    density[0] = counted_magnitudes[magnitudes[0]]\n",
    "    for i, magnitude in enumerate(magnitudes[1:]):\n",
    "        density[i + 1] = density[i] + counted_magnitudes[magnitude]\n",
    "    coefficients, residual, rank, singular_values, rcondition = np.polyfit(\n",
    "        magnitudes, np.log10(density), deg=1, full=True)\n",
    "    b, a = coefficients\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check that we get the same b-value as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = calc_b_value(kaikoura[\"magnitude\"])\n",
    "print(f\"b={b:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling windows with Pandas\n",
    "\n",
    "Pandas has neat ways of doing rolling windows.  We will use this to do two things:\n",
    "1. Calculate the median date for every 2000 earthquakes;\n",
    "2. Calculate the b-value for every 2000 earthquakes.\n",
    "\n",
    "We will then plot these and see if we see any variations.\n",
    "\n",
    "To calculate the median date we will:\n",
    "1. sort the dataframe by `\"origintime\"`\n",
    "2. Extract just the `\"origintime\"` and `\"magnitude\"` columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2000\n",
    "\n",
    "kaikoura = kaikoura.sort_values(by=[\"origintime\"], ignore_index=True)\n",
    "magnitude_times = pd.concat([kaikoura[\"origintime\"], kaikoura[\"magnitude\"]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Make a new column containing the seconds since the first event - pandas doesn't have a simple way to calculate the median of a range of datetimes, so we will change to working in seconds since a reference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds_offset = (magnitude_times.origintime - magnitude_times.origintime[0]).dt.total_seconds()\n",
    "magnitude_times = magnitude_times.merge(\n",
    "    seconds_offset.rename(\"seconds_offset\"), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compute the rolling median of the seconds_offset column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_median = magnitude_times.seconds_offset.rolling(window_size).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Convert this column to timedelta objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_median = pd.to_timedelta(window_median, unit=\"S\") # Unit is seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Add the reference time to these to get back to real-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_median += magnitude_times.origintime[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Put this into the dataframe as a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitude_times = magnitude_times.merge(\n",
    "    window_median.rename(\"window_median\"), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing moving window b-values\n",
    "\n",
    "Computing the moving b-value is a little simpler to write, but slower to run.  We will use the function we wrote above and pandas `.rolling().apply(func)` chained method to apply our custom `func` to our column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_values = magnitude_times.magnitude.rolling(window_size).apply(calc_b_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets quickly convert those from gradients to b-values by multiplying by -1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_values *= -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put those back into the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitude_times = magnitude_times.merge(\n",
    "    b_values.rename(\"b_value\"), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results\n",
    "\n",
    "Now lets plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = magnitude_times.plot(x=\"window_median\", y=\"b_value\", kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What next?\n",
    "\n",
    "There are some pretty impressive variations there! In particular there are strong variations in 2013 and 2016, right around when the Cook Strait and Kaikoura earthquakes happened. I wonder if there is anything in that...? **before we get ahead of ourselves**, we missed some key things here that mean that this result is not interpretable:\n",
    "1. Not all magnitudes are equal, and we were just using GeoNet's summary magnitude;\n",
    "2. We fixed the magnitude of completeness when in reality completeness depends on a range of factors and is time-varying;\n",
    "3. We haven't taken spatial variations into account - we have looked at quite a large region here.\n",
    "\n",
    "We could get around those factors though and extend our rolling window to compute completeness alongside b-value. Potential student project...?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Using pandas rolling windows, find the mean earthquake location for every window we used above. You will need to compute the rolling mean for latitude, longitude and depth.  Make three plots to show how latitude, longitude and depth vary with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
